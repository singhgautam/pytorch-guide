{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code-Snippets for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest\n",
    "Nice functionality to run test-cases on certain modules of your model.\n",
    "Add the following content to a Python file and name it starting with 'test\\_'. \n",
    "Every test function name will also begin with 'test\\_'.\n",
    "\n",
    "#### Parametrize\n",
    "One can parametrize the test cases so they won't need to be 'copied' multiple times. Notice the 3 parameters provided in the decorator and their values as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def pow(base, exponent):\n",
    "    return base ** exponent\n",
    "\n",
    "@pytest.mark.parametrize('a, b, expected',[\n",
    "    (1,2,3),\n",
    "    (4,5,9),\n",
    "    (3,4,7)\n",
    "])\n",
    "def test_sum(a, b, expected):\n",
    "    assert sum(a,b) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixtures\n",
    "Used for executing a step before the testing can commence. Notice the syntax of the decorator below. The function that is decorated with a fixture is something that will be executed whenever that function name is supplied as an argument to a test function. The return value substitutes for the argument of the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def base():\n",
    "    return 4\n",
    "\n",
    "def test_pow(base):\n",
    "    assert pow(base,1) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n",
    "Use the following command in the same directory as the above Python file. The flag '-v' indicates that a verbose output be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Logging\n",
    "Use one of the following functions to print the progress during training.\n",
    "### Batch Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\tBATCH [================                        ]: 40 (Loss: 334.2300)\n",
      "\r",
      "\tBATCH [==================                      ]: 45 (Loss: 334.2300)\n"
     ]
    }
   ],
   "source": [
    "def batch_progress_bar(batch_num, report_interval, last_loss):\n",
    "    \"\"\"Prints the progress until the next report.\"\"\"\n",
    "    progress = (((batch_num-1.0) % report_interval) + 1.0) / report_interval\n",
    "    fill = int(progress * 40)\n",
    "    print \"\\r\\tBATCH [{}{}]: {} (Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \n",
    "        \" \" * (40 - fill), \n",
    "        batch_num, \n",
    "        last_loss)\n",
    "\n",
    "batch_progress_bar(40,100,334.23)\n",
    "batch_progress_bar(45,100,334.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "EPOCH [=========                               ]: 24 (Avg. Training Loss: 334.2300) (Test Loss: 352.4500)\n",
      "\r",
      "EPOCH [============                            ]: 30 (Avg. Training Loss: 334.2300) (Test Loss: 352.4500)\n"
     ]
    }
   ],
   "source": [
    "def epoch_progress_bar(epoch_num, total_epochs, train_loss, test_loss):\n",
    "    \"\"\"Prints the progress until the next report.\"\"\"\n",
    "    progress = (((epoch_num-1.0) % total_epochs) + 1.0) / total_epochs\n",
    "    fill = int(progress * 40)\n",
    "    print \"\\rEPOCH [{}{}]: {} (Avg. Training Loss: {:.4f}) (Test Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \n",
    "        \" \" * (40 - fill), \n",
    "        epoch_num, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "    )\n",
    "\n",
    "epoch_progress_bar(24,100,334.23,352.45)\n",
    "epoch_progress_bar(30,100,334.23,352.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_for_batch(checkpoint_path,\n",
    "                              model,\n",
    "                              name,\n",
    "                              seed,\n",
    "                              epoch_num,\n",
    "                              batch_num,\n",
    "                              losses,\n",
    "                              costs,\n",
    "                              seq_lengths):\n",
    "    basename = \"{}/{}-{}-epoch-{}-batch-{}\".format(checkpoint_path, name, seed, epoch_num, batch_num)\n",
    "    model_fname = basename + \".model\"\n",
    "    LOGGER.info(\"Saving model checkpoint to: '%s'\", model_fname)\n",
    "    torch.save(model.state_dict(), model_fname)\n",
    "\n",
    "    # Save the training history for batch\n",
    "    train_fname = basename + \".json\"\n",
    "    LOGGER.info(\"Saving model training history to '%s'\", train_fname)\n",
    "    content = {\n",
    "        \"loss\": losses,\n",
    "        \"cost\": costs,\n",
    "        \"seq_lengths\": seq_lengths\n",
    "    }\n",
    "    open(train_fname, 'wt').write(json.dumps(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "-- To be added --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tricks\n",
    "### Clipping\n",
    "Clips the the gradient to lie within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_grads(model, range):\n",
    "    \"\"\"Gradient clipping to the range.\"\"\"\n",
    "    parameters = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-range, range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Conventions\n",
    "### Indexing in Sequential Data\n",
    "In this section, we'll look at how to organise sequential data in a tensor in order for it to be a proper input to the model.\n",
    "There are 3 dimensions of interest in the input tensor.\n",
    "1. Sequence length: length of the sequence, say, in terms of time or time-steps.\n",
    "2. Batch size: number of sequences to process in a batch\n",
    "3. Input size: size of the input vector per time-step\n",
    "\n",
    "\n",
    "We'll typically follow the above convention for the input data tensor. To justify this, we enumerate some nice properties that our input data tensor should have.\n",
    "1. Data for each time-step will be processed one after the other by the same model-cell. Model-cell here refers to any generic neural network cell that processes data for one time-step such as a simple LSTM cell or something more complicated like a NTM cell. We typically want to call the model-cell in a loop for input tensors of every time step. This loop is the outer-most artifact in the code-organisation and hence time-step should be the 0th dimension of the input data tensor.\n",
    "2. At a given time-step, when the model-cell is processing a sub-tensor, we'd like to have its 0th dimension as the dimension for the batch index and the 1st dimension as the dimension to index over entries of the input vector.\n",
    "\n",
    "\n",
    "As is evident, the preceding order of dimensions aligns nicely with our requirements and hence would be our convention in ordering our indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size() torch.Size([10, 20, 30])\n",
      "t = 0 \tx.size() torch.Size([20, 30])\n",
      "t = 0 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 1 \tx.size() torch.Size([20, 30])\n",
      "t = 1 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 2 \tx.size() torch.Size([20, 30])\n",
      "t = 2 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 3 \tx.size() torch.Size([20, 30])\n",
      "t = 3 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 4 \tx.size() torch.Size([20, 30])\n",
      "t = 4 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 5 \tx.size() torch.Size([20, 30])\n",
      "t = 5 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 6 \tx.size() torch.Size([20, 30])\n",
      "t = 6 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 7 \tx.size() torch.Size([20, 30])\n",
      "t = 7 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 8 \tx.size() torch.Size([20, 30])\n",
      "t = 8 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 9 \tx.size() torch.Size([20, 30])\n",
      "t = 9 \ty.size() torch.Size([20, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# number of timesteps\n",
    "num_timesteps = 10\n",
    "\n",
    "# number of input vectors per batch\n",
    "batch_size = 20\n",
    "\n",
    "# size of each input vector\n",
    "inp_dim = 30\n",
    "\n",
    "# size of each input vector\n",
    "outp_dim = 2\n",
    "\n",
    "# instantiating the input data tensor\n",
    "X = torch.randn(num_timesteps, batch_size, inp_dim)\n",
    "print 'X.size()', X.size()\n",
    "\n",
    "# instantiating a simple model cell\n",
    "model_cell = nn.Linear(inp_dim, outp_dim)\n",
    "\n",
    "# calling the model in a loop over timesteps\n",
    "for t in range(num_timesteps):\n",
    "    x = X[t] #sub-tensor for this timestep\n",
    "    print 't =',t, '\\tx.size()', x.size()\n",
    "    y = model_cell(x)\n",
    "    print 't =',t, '\\ty.size()', y.size(), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "1. The loop is run over the timesteps and it is convenient for time-step to be the first dimension.\n",
    "2. Size of X[t] or x is (20,30) where 20 is the number of batches and 30 is the length of the input vector.\n",
    "3. Size of y for any time-step t is (20,2) where 20 is the number of batches and 2 is the length of the output vector. Hence we can take-away that (batch_idx, inp_dim) is the convention that is followed by the pre-written modules of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuances of Squeeze and Unsqueeze\n",
    "\n",
    "Any pre-defined module in PyTorch would consume a tensor of a fixed dimensionality. To comply with this, you have to massage your tensor to have the exact same size as defined by the PyTorch documentations. Furthermore, each dimension itself must represent what it ideally should according to the PyTorch documentations.\n",
    "\n",
    "- **Example 1:** An LSTM module consumes as input a 3 dimensional tensor with each dimension meaning *(sequence, batch, values)*. Suppose you were interested in processing just one time-step and you actually had a 2D tensor having *(batch, values)*. In this case, you would **unsqueeze** the 2D tensor **at the dimension 0** to result in a 3D tensor such that the first dimension represents time-steps with just one time-step entry in it.\n",
    "- **Example 2:** An LSTM module's output is a 3 dimensional tensor with the dimensions standing for *(sequence, batch, values)*. Suppose you gave the input as a 3D tensor for just one time step as exemplified in the above Example 1. But after getting the output, you want to do other things with the output and the 3D output tensor having semantics *(sequence, batch, values)* seems a bit cumbersome because the *sequence* dimension is just size 1 since we are dealing with just one time step. So we'd use squeeze to strip it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Batches and Epochs\n",
    "\n",
    "1. When the **training data is too small** and you still decide to train a deep network, you may not want to bother with batches at all. Essentially, each iteration would consider the entire training data and each iteration will be like an epoch. So epoch as a term loses its meaning.\n",
    "2. When the **training data is big but limited**, then you want to run each iteration in batches and you still want to see the entire data set more than once in the form of epochs.\n",
    "3. When the **training data is unlimited**, which happens when you are synthetically generating the data, then you many not want to bother with epochs at all. You can just call up a fixed number of batches with some defined size of each batch. And you run the iterations per batch and each batch is only seen once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_parameters(self, stdv=1e-1):\n",
    "    for weight in self.parameters():\n",
    "        weight.data.normal_(0, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuggets of Python Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attrs and Attrib\n",
    "Inline code-autocompleter in Python. Decorators are used to do this. Use '@attrs' for classes and use 'attrib()' for specifying the attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rectangle(length=30.0, breadth=30.0)\n",
      "Rectangle(length=10.0, breadth=50.0)\n",
      "Rectangle(length=100.0, breadth=50.0)\n",
      "{'breadth': 30.0, 'length': 30.0}\n",
      "100.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "import attr\n",
    "from attr import attrs, attrib, Factory\n",
    "\n",
    "@attrs\n",
    "class Rectangle(object):\n",
    "    length = attrib(default=100, converter=float)\n",
    "    breadth = attrib(default=50, converter=float)\n",
    "    \n",
    "print Rectangle(length=30, breadth=30)\n",
    "print Rectangle(length=10)\n",
    "print Rectangle()\n",
    "print attr.asdict(Rectangle(length=30, breadth=30))\n",
    "\n",
    "@attrs\n",
    "class TwoRectangles(object):\n",
    "    rect1 = attrib(factory=Rectangle)\n",
    "    rect2 = attrib(factory=Rectangle)\n",
    "print TwoRectangles().rect1.length\n",
    "print TwoRectangles().rect1.breadth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### rstrip\n",
    "If you have a path and you want to strip away the trailing '/' that people sometimes specify, use the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gautam/myfolder\n"
     ]
    }
   ],
   "source": [
    "print '/home/gautam/myfolder/////'.rstrip('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data Sets in PyTorch\n",
    "\n",
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.size() torch.Size([1, 1, 28, 28])\n",
      "label 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# train data set\n",
    "train_dataset = datasets.MNIST('data',\n",
    "               train=True,\n",
    "               download=True,\n",
    "               transform=transforms.ToTensor())\n",
    "\n",
    "# test data set\n",
    "test_dataset = datasets.MNIST('data',\n",
    "               train=False,\n",
    "               transform=transforms.ToTensor())\n",
    "\n",
    "# train data generator\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "train_loader_iter = cycle(train_loader) # cycles over the entire data set to generate the samples\n",
    "\n",
    "# test data generator\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "test_loader_iter = iter(test_loader) # iterates over the entire data set only once\n",
    "\n",
    "# generate a training data samples\n",
    "_data, _label = train_loader_iter.next()\n",
    "plt.imshow(_data.squeeze().numpy(), cmap='gray')\n",
    "print 'data.size()\\t\\t', _data.size()\n",
    "print 'label\\t\\t', _label.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
