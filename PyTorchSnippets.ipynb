{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code-Snippets for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest\n",
    "Nice functionality to run test-cases on certain modules of your model.\n",
    "Add the following content to a Python file and name it starting with 'test\\_'. \n",
    "Every test function name will also begin with 'test\\_'.\n",
    "\n",
    "#### Parametrize\n",
    "One can parametrize the test cases so they won't need to be 'copied' multiple times. Notice the 3 parameters provided in the decorator and their values as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "def pow(base, exponent):\n",
    "    return base ** exponent\n",
    "\n",
    "@pytest.mark.parametrize('a, b, expected',[\n",
    "    (1,2,3),\n",
    "    (4,5,9),\n",
    "    (3,4,7)\n",
    "])\n",
    "def test_sum(a, b, expected):\n",
    "    assert sum(a,b) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixtures\n",
    "Used for executing a step before the testing can commence. Notice the syntax of the decorator below. The function that is decorated with a fixture is something that will be executed whenever that function name is supplied as an argument to a test function. The return value substitutes for the argument of the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def base():\n",
    "    return 4\n",
    "\n",
    "def test_pow(base):\n",
    "    assert pow(base,1) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n",
    "Use the following command in the same directory as the above Python file. The flag '-v' indicates that a verbose output be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Logging\n",
    "Use one of the following functions to print the progress during training.\n",
    "### Batch Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\tBATCH [================                        ]: 40 (Loss: 334.2300)\n",
      "\r",
      "\tBATCH [==================                      ]: 45 (Loss: 334.2300)\n"
     ]
    }
   ],
   "source": [
    "def batch_progress_bar(batch_num, report_interval, last_loss):\n",
    "    \"\"\"Prints the progress until the next report.\"\"\"\n",
    "    progress = (((batch_num-1.0) % report_interval) + 1.0) / report_interval\n",
    "    fill = int(progress * 40)\n",
    "    print \"\\r\\tBATCH [{}{}]: {} (Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \n",
    "        \" \" * (40 - fill), \n",
    "        batch_num, \n",
    "        last_loss)\n",
    "\n",
    "batch_progress_bar(40,100,334.23)\n",
    "batch_progress_bar(45,100,334.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "EPOCH [=========                               ]: 24 (Avg. Training Loss: 334.2300) (Test Loss: 352.4500)\n",
      "\r",
      "EPOCH [============                            ]: 30 (Avg. Training Loss: 334.2300) (Test Loss: 352.4500)\n"
     ]
    }
   ],
   "source": [
    "def epoch_progress_bar(epoch_num, total_epochs, train_loss, test_loss):\n",
    "    \"\"\"Prints the progress until the next report.\"\"\"\n",
    "    progress = (((epoch_num-1.0) % total_epochs) + 1.0) / total_epochs\n",
    "    fill = int(progress * 40)\n",
    "    print \"\\rEPOCH [{}{}]: {} (Avg. Training Loss: {:.4f}) (Test Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \n",
    "        \" \" * (40 - fill), \n",
    "        epoch_num, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "    )\n",
    "\n",
    "epoch_progress_bar(24,100,334.23,352.45)\n",
    "epoch_progress_bar(30,100,334.23,352.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint_for_batch(checkpoint_path,\n",
    "                              model,\n",
    "                              name,\n",
    "                              seed,\n",
    "                              epoch_num,\n",
    "                              batch_num,\n",
    "                              losses,\n",
    "                              costs,\n",
    "                              seq_lengths):\n",
    "    basename = \"{}/{}-{}-epoch-{}-batch-{}\".format(checkpoint_path, name, seed, epoch_num, batch_num)\n",
    "    model_fname = basename + \".model\"\n",
    "    LOGGER.info(\"Saving model checkpoint to: '%s'\", model_fname)\n",
    "    torch.save(model.state_dict(), model_fname)\n",
    "\n",
    "    # Save the training history for batch\n",
    "    train_fname = basename + \".json\"\n",
    "    LOGGER.info(\"Saving model training history to '%s'\", train_fname)\n",
    "    content = {\n",
    "        \"loss\": losses,\n",
    "        \"cost\": costs,\n",
    "        \"seq_lengths\": seq_lengths\n",
    "    }\n",
    "    open(train_fname, 'wt').write(json.dumps(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tricks\n",
    "### Clipping\n",
    "Clips the the gradient to lie within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_grads(model, range):\n",
    "    \"\"\"Gradient clipping to the range.\"\"\"\n",
    "    parameters = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-range, range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Conventions\n",
    "### Indexing in Sequential Data\n",
    "In this section, we'll look at how to organise sequential data in a tensor in order for it to be a proper input to the model.\n",
    "There are 3 dimensions of interest in the input tensor.\n",
    "1. Sequence length: length of the sequence, say, in terms of time or time-steps.\n",
    "2. Batch size: number of sequences to process in a batch\n",
    "3. Input size: size of the input vector per time-step\n",
    "\n",
    "\n",
    "We'll typically follow the above convention for the input data tensor. To justify this, we enumerate some nice properties that our input data tensor should have.\n",
    "1. Data for each time-step will be processed one after the other by the same model-cell. Model-cell here refers to any generic neural network cell that processes data for one time-step such as a simple LSTM cell or something more complicated like a NTM cell. We typically want to call the model-cell in a loop for input tensors of every time step. This loop is the outer-most artifact in the code-organisation and hence time-step should be the 0th dimension of the input data tensor.\n",
    "2. When a sub-tensor is being processed by the model cell, we'd like to have its 0th dimension as the dimension for the batch index and the 1st dimension as the dimension to index over entries of the input vector\n",
    "<br>\n",
    "As is evident, the preceding order of dimensions aligns nicely with our requirements and hence would be our convention in ordering our indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size() torch.Size([10, 20, 30])\n",
      "t = 0 \tx.size() torch.Size([20, 30])\n",
      "t = 0 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 1 \tx.size() torch.Size([20, 30])\n",
      "t = 1 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 2 \tx.size() torch.Size([20, 30])\n",
      "t = 2 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 3 \tx.size() torch.Size([20, 30])\n",
      "t = 3 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 4 \tx.size() torch.Size([20, 30])\n",
      "t = 4 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 5 \tx.size() torch.Size([20, 30])\n",
      "t = 5 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 6 \tx.size() torch.Size([20, 30])\n",
      "t = 6 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 7 \tx.size() torch.Size([20, 30])\n",
      "t = 7 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 8 \tx.size() torch.Size([20, 30])\n",
      "t = 8 \ty.size() torch.Size([20, 2]) \n",
      "\n",
      "t = 9 \tx.size() torch.Size([20, 30])\n",
      "t = 9 \ty.size() torch.Size([20, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# number of timesteps\n",
    "num_timesteps = 10\n",
    "\n",
    "# number of input vectors per batch\n",
    "batch_size = 20\n",
    "\n",
    "# size of each input vector\n",
    "inp_dim = 30\n",
    "\n",
    "# size of each input vector\n",
    "outp_dim = 2\n",
    "\n",
    "# instantiating the input data tensor\n",
    "X = torch.randn(num_timesteps, batch_size, inp_dim)\n",
    "print 'X.size()', X.size()\n",
    "\n",
    "# instantiating a simple model cell\n",
    "model_cell = nn.Linear(inp_dim, outp_dim)\n",
    "\n",
    "# calling the model in a loop over timesteps\n",
    "for t in range(num_timesteps):\n",
    "    x = X[t] #sub-tensor for this timestep\n",
    "    print 't =',t, '\\tx.size()', x.size()\n",
    "    y = model_cell(x)\n",
    "    print 't =',t, '\\ty.size()', y.size(), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "1. The loop is run over the timesteps and it is convenient for time-step to be the first dimension.\n",
    "2. Size of X[t] or x is (20,30) where 20 is the number of batches and 30 is the length of the input vector.\n",
    "3. Size of y for any time-step t is (20,2) where 20 is the number of batches and 2 is the length of the output vector. Hence we can take-away that (batch_idx, inp_dim) is the convention that is followed by the pre-written modules of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
